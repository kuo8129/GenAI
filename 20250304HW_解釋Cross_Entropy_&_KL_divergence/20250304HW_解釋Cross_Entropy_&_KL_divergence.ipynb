{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPu7CRUb1WFvQV6mlyQ5Br",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuo8129/GenAI/blob/main/20250304HW_%E8%A7%A3%E9%87%8BCross_Entropy_%26_KL_divergence/20250304HW_%E8%A7%A3%E9%87%8BCross_Entropy_%26_KL_divergence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 何謂Entropy (熵)？\n",
        "Entropy為平均資訊量，又稱「亂度」，可用來衡量隨機變數的不確定性。\n",
        "\n",
        "定義公式如下：\n",
        "\n",
        "$$\n",
        "H(X) = -\\sum p(x) \\log p(x)\n",
        "$$\n",
        "\n",
        "其中，\n",
        "* $p(x)$ 為事件 $x$ 發生的機率\n",
        "\n",
        "Entropy越大，平均資訊量越大，代表可能的情況越複雜；反之，若只有一個事件，發生的機率為1，則Entropy為0。"
      ],
      "metadata": {
        "id": "VzEF49pvSMgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 何謂Cross Entropy (交叉熵)？\n",
        "Cross Entropy是一個用來衡量兩個機率分布之間差異的指標，可以作為衡量模型預測的結果和真實結果之間差距的損失函數(loss function)。\n",
        "\n",
        "與Entropy的不同在於：Cross Entropy將 $p(x) \\log p(x)$ 改成 $p(x) \\log q(x)$ ，表示使用模型的預測來描述真實的機率分佈。\n",
        "\n",
        "其定義公式如下：\n",
        "\n",
        "$$\n",
        "H(P, Q) = - \\sum p(x) \\log q(x)\n",
        "$$\n",
        "\n",
        "其中，\n",
        "* $p(x)$ 是真實的機率分佈\n",
        "* $q(x)$ 是模型預測的機率分佈\n",
        "\n",
        "若模型預測的結果和真實結果相差甚遠，則Cross Entropy的值就會很大，代表模型預測的錯誤情況越大；反之，若模型預測的結果和真實結果很接近，Cross Entropy的值就會很小，代表模型預測越精準。\n",
        "\n",
        "當 $Q=P$ 時，$H(P, Q)$ 有最小值 $H(P)$ ，$H(P)$ 不一定為0。"
      ],
      "metadata": {
        "id": "gFc7OdrR6WCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 何謂Kullback-Leibler Divergence (KL Divergence、KL散度)？\n",
        "KL Divergence是一個用來衡量兩個機率分佈相似度或差異度的指標。\n",
        "\n",
        "與Cross Entropy的不同在於：KL Divergence等於Cross Entropy再減掉真實世界的Entropy。\n",
        "\n",
        "其定義公式如下：\n",
        "\n",
        "$$\n",
        "D_{KL}(P||Q) = \\sum p(x) \\log \\frac{p(x)}{q(x)} = H(P, Q) - H(P)\n",
        "$$\n",
        "\n",
        "其中，\n",
        "* $p(x)$ 是真實的機率分佈\n",
        "* $q(x)$ 是模型預測的機率分佈\n",
        "\n",
        "若模型預測的機率分佈與真實的機率分佈相差很多時，KL Divergence就越大，代表差異很大；而若模型預測的機率分佈與真實的機率分佈很接近時，KL Divergence就越小，代表差異很小。\n",
        "\n",
        "當 $Q=P$ 時，$H(P, Q) = H(P)$，此時 $D_{KL}(P||Q)$ 會有最小值0。"
      ],
      "metadata": {
        "id": "glPEIjTg6fBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 適用時機\n",
        "\n",
        "### **Cross Entropy**\n",
        "\n",
        "由於相較於均方差(Mean Squared Error, MSE)而言，Cross Entropy可直接衡量模型預測結果與真實結果的差異，並對錯誤預測給予較大的懲罰，收斂速度較快，能幫助模型更有效率地進行學習，因此常被用於分類問題，特別是One-Hot Encoding的分類問題。\n",
        "\n",
        "例如：圖像分類\n",
        "\n",
        "### **KL Divergence**\n",
        "\n",
        "當不僅關心預測結果是否正確，還希望模型預測的分佈更貼近目標分佈時，或是模型輸出的是一機率分佈，而非單一答案的情況下，就會使用KL Divergence來進行評估。\n",
        "\n",
        "例如：知識蒸餾\n",
        "\n",
        "### **何時皆可用？**\n",
        "\n",
        "當分類問題是以One-Hot Encoding的方式呈現時，運用Cross Entropy或KL Divergence來衡量，結果皆相同，因為 $p(x)$ 只有在正確的類別會=1，其餘皆=0，因此兩種方法皆可使用，不過因為Cross Entropy運算較簡單，不須額外再減掉真實世界的Entropy，故大部分仍用Cross Entropy來進行衡量。"
      ],
      "metadata": {
        "id": "wV3lwV-x66ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "P = np.array([1, 0, 0])\n",
        "Q = np.array([0.7, 0.2, 0.1])\n",
        "\n",
        "cross_entropy = -np.sum(P * np.log(Q))\n",
        "kl_divergence = np.sum(np.where(P != 0, P * np.log(P / Q), 0)) # 以0替代無意義的0log0，避免影響結果\n",
        "\n",
        "# 列印計算過程\n",
        "print(f\"H(P,Q) = - (1 * log({Q[0]}) + 0 * log({Q[1]}) + 0 * log({Q[2]}))\")\n",
        "print(f\"          = - log({Q[0]})\")\n",
        "print(f\"          = {cross_entropy:.4f}\\n\")\n",
        "\n",
        "print(f\"D_KL(P||Q) = 1 * log(1/{Q[0]}) + 0 * log(0/{Q[1]}) + 0 * log(0/{Q[2]})\")\n",
        "print(f\"             = log(1) - log({Q[0]})\")\n",
        "print(f\"             = - log({Q[0]})\")\n",
        "print(f\"             = {kl_divergence:.4f}\\n\")\n",
        "\n",
        "print(f\"計算結果: Cross Entropy = {cross_entropy:.4f}, KL Divergence = {kl_divergence:.4f} ({'兩者相等' if np.isclose(cross_entropy, kl_divergence) else '兩者不相等'})\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQV3F_D9owCN",
        "outputId": "93826798-7764-4f7c-cf91-76e92a184bca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "H(P,Q) = - (1 * log(0.7) + 0 * log(0.2) + 0 * log(0.1))\n",
            "          = - log(0.7)\n",
            "          = 0.3567\n",
            "\n",
            "D_KL(P||Q) = 1 * log(1/0.7) + 0 * log(0/0.2) + 0 * log(0/0.1)\n",
            "             = log(1) - log(0.7)\n",
            "             = - log(0.7)\n",
            "             = 0.3567\n",
            "\n",
            "計算結果: Cross Entropy = 0.3567, KL Divergence = 0.3567 (兩者相等)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-93c5f00a1b4c>:10: RuntimeWarning: divide by zero encountered in log\n",
            "  kl_divergence = np.sum(np.where(P != 0, P * np.log(P / Q), 0)) # 以0替代無意義的0log0，避免影響結果\n",
            "<ipython-input-1-93c5f00a1b4c>:10: RuntimeWarning: invalid value encountered in multiply\n",
            "  kl_divergence = np.sum(np.where(P != 0, P * np.log(P / Q), 0)) # 以0替代無意義的0log0，避免影響結果\n"
          ]
        }
      ]
    }
  ]
}